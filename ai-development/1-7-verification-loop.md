---
layout: default
title: "1-7: ê²€ì¦ ë£¨í”„ êµ¬í˜„ - GPT-5ë¥¼ ì½”ë“œ ë¦¬ë·°ì–´ë¡œ í™œìš©í•˜ì—¬ ê²°ê³¼ë¬¼ í’ˆì§ˆ ë³´ì¥í•˜ê¸°"
description: "ì—ì´ì „í‹± SaaS ì¡°ì§ ê°€ì´ë“œ"
order: 9
permalink: /ai-development/1-7-verification-loop/
---

# 1-7: ê²€ì¦ ë£¨í”„ êµ¬í˜„ - GPT-5ë¥¼ ì½”ë“œ ë¦¬ë·°ì–´ë¡œ í™œìš©í•˜ì—¬ ê²°ê³¼ë¬¼ í’ˆì§ˆ ë³´ì¥í•˜ê¸°

## ğŸ“‹ ê°œìš”

ê²€ì¦ ë£¨í”„ëŠ” AI ì—ì´ì „íŠ¸ê°€ ìƒì„±í•œ ê²°ê³¼ë¬¼ì˜ í’ˆì§ˆì„ ìë™ìœ¼ë¡œ ê²€ì¦í•˜ê³  ê°œì„ í•˜ëŠ” í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. GPT-5ë¥¼ ì½”ë“œ ë¦¬ë·°ì–´ë¡œ í™œìš©í•˜ì—¬ ì§€ì†ì ì¸ í’ˆì§ˆ í–¥ìƒì„ ë‹¬ì„±í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ

ì´ ê°€ì´ë“œë¥¼ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **ê²€ì¦ ë£¨í”„ì˜ ì›ë¦¬ì™€ ì¤‘ìš”ì„± ì´í•´**
2. **GPT-5 ê¸°ë°˜ ìë™ ì½”ë“œ ë¦¬ë·° ì‹œìŠ¤í…œ êµ¬ì¶•**
3. **í’ˆì§ˆ ì§€í‘œì™€ ê°œì„  ë©”ì»¤ë‹ˆì¦˜ ì„¤ê³„**
4. **ì‹¤ì œ í”„ë¡œì íŠ¸ì— ê²€ì¦ ë£¨í”„ ì ìš©**

## ğŸ”„ ê²€ì¦ ë£¨í”„ì˜ í•µì‹¬ ì›ë¦¬

### ê²€ì¦ ë£¨í”„ê°€ í•„ìš”í•œ ì´ìœ 

#### AI ìƒì„± ì½”ë“œì˜ ë¬¸ì œì 
- **ì¼ê´€ì„± ë¶€ì¡±**: ê°™ì€ ìš”ì²­ë„ ë§¤ë²ˆ ë‹¤ë¥¸ ê²°ê³¼
- **í’ˆì§ˆ í¸ì°¨**: ë•Œë¡œëŠ” í›Œë¥­í•˜ì§€ë§Œ ë•Œë¡œëŠ” í˜•í¸ì—†ìŒ
- **ê²€ì¦ ì–´ë ¤ì›€**: ìì²´ ê²€ì¦ì´ ì–´ë ¤ì›€
- **ê°œì„  ë¶€ì¡±**: ì‹¤íŒ¨ì—ì„œ í•™ìŠµí•˜ì§€ ëª»í•¨

#### ê²€ì¦ ë£¨í”„ì˜ ì¥ì 
- **í’ˆì§ˆ ë³´ì¥**: ì¼ê´€ëœ ê³ í’ˆì§ˆ ê²°ê³¼ë¬¼
- **ìë™ ê°œì„ **: í”¼ë“œë°±ì„ í†µí•œ ì§€ì†ì  í–¥ìƒ
- **ì‹ ë¢°ì„±**: ê²€ì¦ëœ ê²°ê³¼ë¬¼ì— ëŒ€í•œ ì‹ ë¢°
- **íš¨ìœ¨ì„±**: ìˆ˜ë™ ê²€í†  ì‹œê°„ ë‹¨ì¶•

### ê²€ì¦ ë£¨í”„ ì•„í‚¤í…ì²˜

```mermaid
graph TD
    A[ì½”ë“œ ìƒì„±] --> B[GPT-5 ê²€ì¦ì]
    B --> C{í’ˆì§ˆ ê²€ì¦}
    C -->|í†µê³¼| D[ìµœì¢… ê²°ê³¼]
    C -->|ì‹¤íŒ¨| E[í”¼ë“œë°± ìƒì„±]
    E --> F[ê°œì„ ëœ ì½”ë“œ ìƒì„±]
    F --> B
    G[í’ˆì§ˆ ì§€í‘œ] --> B
    H[í•™ìŠµ ë°ì´í„°] --> B
```markdown

## ğŸ› ï¸ GPT-5 ê¸°ë°˜ ê²€ì¦ ì‹œìŠ¤í…œ êµ¬í˜„

### ê¸°ë³¸ ê²€ì¦ ì‹œìŠ¤í…œ

```python
class GPT5CodeReviewer:
    def __init__(self, api_key):
        self.client = OpenAI(api_key=api_key)
        self.model = "gpt-5"
        self.quality_standards = self.load_quality_standards()
    
    def review_code(self, code, requirements, context):
        review_prompt = self.create_review_prompt(code, requirements, context)
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": review_prompt}],
            temperature=0.3  # ì¼ê´€ëœ ê²€ì¦ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
        )
        
        review_result = self.parse_review_result(response.choices[0].message.content)
        return review_result
    
    def create_review_prompt(self, code, requirements, context):
        return f"""
        ë‹¤ìŒ ì½”ë“œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•˜ê³  í’ˆì§ˆì„ í‰ê°€í•˜ì„¸ìš”:
        
        ## ì½”ë“œ
        ```typescript
        {code}
        ```python
        
        ## ìš”êµ¬ì‚¬í•­
        {requirements}
        
        ## ì»¨í…ìŠ¤íŠ¸
        {context}
        
        ## í’ˆì§ˆ ê¸°ì¤€
        {self.quality_standards}
        
        ë‹¤ìŒ í•­ëª©ë“¤ì„ í‰ê°€í•˜ì„¸ìš”:
        
        ### 1. ê¸°ëŠ¥ì  ì •í™•ì„± (0-100ì )
        - ìš”êµ¬ì‚¬í•­ ì¶©ì¡± ì—¬ë¶€
        - ë¡œì§ì˜ ì •í™•ì„±
        - ì—£ì§€ ì¼€ì´ìŠ¤ ì²˜ë¦¬
        
        ### 2. ì½”ë“œ í’ˆì§ˆ (0-100ì )
        - ê°€ë…ì„±
        - ìœ ì§€ë³´ìˆ˜ì„±
        - ì„±ëŠ¥
        - ë³´ì•ˆ
        
        ### 3. ì•„í‚¤í…ì²˜ ì¤€ìˆ˜ (0-100ì )
        - ì„¤ê³„ ì›ì¹™ ì¤€ìˆ˜
        - íŒ¨í„´ ì¼ê´€ì„±
        - ëª¨ë“ˆí™”
        
        ### 4. í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ì„± (0-100ì )
        - í…ŒìŠ¤íŠ¸ ì‘ì„± ìš©ì´ì„±
        - ì˜ì¡´ì„± ì£¼ì…
        - ê²©ë¦¬ ê°€ëŠ¥ì„±
        
        ê° í•­ëª©ì— ëŒ€í•´ ì ìˆ˜ì™€ êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•˜ê³ , 
        ì „ì²´ì ìœ¼ë¡œ í†µê³¼/ì‹¤íŒ¨ë¥¼ íŒì •í•˜ì„¸ìš”.
        """
    
    def parse_review_result(self, review_text):
        # GPT-5ì˜ ê²€í†  ê²°ê³¼ë¥¼ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜
        lines = review_text.split('\n')
        
        scores = {}
        feedback = []
        overall_pass = False
        
        for line in lines:
            if "ì ìˆ˜:" in line or "ì " in line:
                # ì ìˆ˜ ì¶”ì¶œ
                score_match = re.search(r'(\d+)ì ', line)
                if score_match:
                    category = self.extract_category(line)
                    scores[category] = int(score_match.group(1))
            
            elif "í”¼ë“œë°±:" in line or "ê°œì„ ì‚¬í•­:" in line:
                # í”¼ë“œë°± ì¶”ì¶œ
                feedback.append(line.split(':', 1)[1].strip())
            
            elif "í†µê³¼" in line or "ì‹¤íŒ¨" in line:
                overall_pass = "í†µê³¼" in line
        
        return {
            "passed": overall_pass,
            "scores": scores,
            "feedback": feedback,
            "overall_score": sum(scores.values()) / len(scores) if scores else 0
        }
```markdown

### ê³ ê¸‰ ê²€ì¦ ê¸°ëŠ¥

#### 1. ë‹¤ì°¨ì› í’ˆì§ˆ í‰ê°€

```python
class MultiDimensionalReviewer(GPT5CodeReviewer):
    def __init__(self, api_key):
        super().__init__(api_key)
        self.dimension_weights = {
            "functionality": 0.3,
            "quality": 0.25,
            "architecture": 0.2,
            "testability": 0.15,
            "security": 0.1
        }
    
    def comprehensive_review(self, code, requirements, context):
        # ê° ì°¨ì›ë³„ ìƒì„¸ ê²€í† 
        reviews = {}
        
        for dimension in self.dimension_weights.keys():
            reviews[dimension] = self.review_dimension(
                code, requirements, context, dimension
            )
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = sum(
            reviews[dim]["score"] * weight 
            for dim, weight in self.dimension_weights.items()
        )
        
        # í†µê³¼/ì‹¤íŒ¨ íŒì •
        passed = overall_score >= 70  # 70ì  ì´ìƒ í†µê³¼
        
        return {
            "passed": passed,
            "overall_score": overall_score,
            "dimension_scores": reviews,
            "recommendations": self.generate_recommendations(reviews)
        }
    
    def review_dimension(self, code, requirements, context, dimension):
        dimension_prompts = {
            "functionality": self.get_functionality_prompt(),
            "quality": self.get_quality_prompt(),
            "architecture": self.get_architecture_prompt(),
            "testability": self.get_testability_prompt(),
            "security": self.get_security_prompt()
        }
        
        prompt = dimension_prompts[dimension].format(
            code=code, requirements=requirements, context=context
        )
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        
        return self.parse_dimension_result(response.choices[0].message.content)
```markdown

#### 2. ì ì‘í˜• ê²€ì¦ ê¸°ì¤€

```python
class AdaptiveReviewer(MultiDimensionalReviewer):
    def __init__(self, api_key):
        super().__init__(api_key)
        self.learning_history = []
        self.adaptive_criteria = {}
    
    def adaptive_review(self, code, requirements, context):
        # í”„ë¡œì íŠ¸ íŠ¹ì„±ì— ë§ëŠ” ê²€ì¦ ê¸°ì¤€ ì ìš©
        project_type = self.identify_project_type(requirements)
        
        if project_type in self.adaptive_criteria:
            criteria = self.adaptive_criteria[project_type]
        else:
            criteria = self.get_default_criteria()
            self.adaptive_criteria[project_type] = criteria
        
        # ì ì‘í˜• ê²€ì¦ ìˆ˜í–‰
        review_result = self.review_with_criteria(code, requirements, context, criteria)
        
        # í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
        self.collect_learning_data(code, requirements, review_result)
        
        return review_result
    
    def identify_project_type(self, requirements):
        # ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ í”„ë¡œì íŠ¸ ìœ í˜• ì‹ë³„
        type_indicators = {
            "web_app": ["ì›¹", "ë¸Œë¼ìš°ì €", "ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤"],
            "api": ["API", "ì—”ë“œí¬ì¸íŠ¸", "REST"],
            "mobile": ["ëª¨ë°”ì¼", "ì•±", "iOS", "Android"],
            "data_science": ["ë°ì´í„°", "ë¶„ì„", "ë¨¸ì‹ ëŸ¬ë‹", "AI"]
        }
        
        for project_type, indicators in type_indicators.items():
            if any(indicator in requirements for indicator in indicators):
                return project_type
        
        return "general"
    
    def collect_learning_data(self, code, requirements, review_result):
        learning_data = {
            "code_complexity": self.assess_complexity(code),
            "requirements_clarity": self.assess_requirements_clarity(requirements),
            "review_result": review_result,
            "timestamp": datetime.now()
        }
        
        self.learning_history.append(learning_data)
        
        # í•™ìŠµ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²€ì¦ ê¸°ì¤€ ì—…ë°ì´íŠ¸
        self.update_adaptive_criteria()
```markdown

### í”¼ë“œë°± ìƒì„± ë° ê°œì„  ì‹œìŠ¤í…œ

#### 1. êµ¬ì²´ì  í”¼ë“œë°± ìƒì„±

```python
class FeedbackGenerator:
    def __init__(self, gpt5_api_key):
        self.client = OpenAI(api_key=gpt5_api_key)
        self.model = "gpt-5"
    
    def generate_improvement_feedback(self, code, review_result):
        feedback_prompt = f"""
        ë‹¤ìŒ ì½”ë“œ ê²€í†  ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”:
        
        ## ì›ë³¸ ì½”ë“œ
        ```typescript
        {code}
        ```python
        
        ## ê²€í†  ê²°ê³¼
        {review_result}
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”:
        
        ### ğŸ”´ ì‹¬ê°í•œ ë¬¸ì œ (ì¦‰ì‹œ ìˆ˜ì • í•„ìš”)
        - [ë¬¸ì œì ]: [êµ¬ì²´ì  ì„¤ëª…]
        - [í•´ê²°ë°©ì•ˆ]: [ë‹¨ê³„ë³„ í•´ê²° ë°©ë²•]
        
        ### ğŸŸ¡ ê°œì„  ê¶Œì¥ (í’ˆì§ˆ í–¥ìƒ)
        - [ê°œì„ ì ]: [êµ¬ì²´ì  ì„¤ëª…]
        - [ê°œì„ ë°©ì•ˆ]: [êµ¬ì²´ì  êµ¬í˜„ ë°©ë²•]
        
        ### ğŸŸ¢ ì¶”ê°€ ê³ ë ¤ì‚¬í•­ (ì¥ê¸°ì  ê°œì„ )
        - [ê³ ë ¤ì‚¬í•­]: [êµ¬ì²´ì  ì„¤ëª…]
        - [êµ¬í˜„ë°©ì•ˆ]: [êµ¬ì²´ì  êµ¬í˜„ ë°©ë²•]
        
        ê° í”¼ë“œë°±ì— ëŒ€í•´ ì½”ë“œ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”.
        """
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": feedback_prompt}],
            temperature=0.4
        )
        
        return response.choices[0].message.content
```markdown

#### 2. ìë™ ê°œì„  ì œì•ˆ

```python
class AutoImprovementSystem:
    def __init__(self, feedback_generator, code_generator):
        self.feedback_generator = feedback_generator
        self.code_generator = code_generator
    
    def suggest_improvements(self, code, review_result):
        # í”¼ë“œë°± ìƒì„±
        feedback = self.feedback_generator.generate_improvement_feedback(
            code, review_result
        )
        
        # ê°œì„ ëœ ì½”ë“œ ìƒì„±
        improved_code = self.code_generator.improve_code(code, feedback)
        
        # ê°œì„  ì‚¬í•­ ê²€ì¦
        improvement_verification = self.verify_improvements(
            code, improved_code, review_result
        )
        
        return {
            "original_code": code,
            "improved_code": improved_code,
            "feedback": feedback,
            "improvement_verification": improvement_verification
        }
    
    def verify_improvements(self, original_code, improved_code, original_review):
        verification_prompt = f"""
        ë‹¤ìŒ ì½”ë“œ ê°œì„ ì´ ì›ë³¸ ê²€í†  ê²°ê³¼ì˜ ë¬¸ì œì ì„ í•´ê²°í–ˆëŠ”ì§€ ê²€ì¦í•˜ì„¸ìš”:
        
        ## ì›ë³¸ ì½”ë“œ
        ```typescript
        {original_code}
        ```python
        
        ## ê°œì„ ëœ ì½”ë“œ
        ```typescript
        {improved_code}
        ```python
        
        ## ì›ë³¸ ê²€í†  ê²°ê³¼
        {original_review}
        
        ë‹¤ìŒì„ í‰ê°€í•˜ì„¸ìš”:
        1. ì›ë³¸ ë¬¸ì œì ì´ í•´ê²°ë˜ì—ˆëŠ”ê°€?
        2. ìƒˆë¡œìš´ ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ëŠ”ê°€?
        3. ì „ì²´ì ì¸ í’ˆì§ˆì´ í–¥ìƒë˜ì—ˆëŠ”ê°€?
        4. ì„±ëŠ¥ì— ë¶€ì •ì  ì˜í–¥ì„ ì£¼ì§€ ì•Šì•˜ëŠ”ê°€?
        
        ê° í•­ëª©ì— ëŒ€í•´ í†µê³¼/ì‹¤íŒ¨ë¥¼ íŒì •í•˜ê³  ì ìˆ˜ë¥¼ ì œê³µí•˜ì„¸ìš”.
        """
        
        response = self.client.chat.completions.create(
            model="gpt-5",
            messages=[{"role": "user", "content": verification_prompt}],
            temperature=0.3
        )
        
        return self.parse_verification_result(response.choices[0].message.content)
```markdown

## ğŸ“Š í’ˆì§ˆ ì§€í‘œ ë° ëª¨ë‹ˆí„°ë§

### í•µì‹¬ í’ˆì§ˆ ì§€í‘œ

#### 1. ì½”ë“œ í’ˆì§ˆ ì§€í‘œ
```python
class QualityMetrics:
    def __init__(self):
        self.metrics = {
            "functionality_score": 0,
            "quality_score": 0,
            "architecture_score": 0,
            "testability_score": 0,
            "security_score": 0,
            "overall_score": 0
        }
    
    def calculate_metrics(self, review_result):
        scores = review_result.get("scores", {})
        
        self.metrics["functionality_score"] = scores.get("functionality", 0)
        self.metrics["quality_score"] = scores.get("quality", 0)
        self.metrics["architecture_score"] = scores.get("architecture", 0)
        self.metrics["testability_score"] = scores.get("testability", 0)
        self.metrics["security_score"] = scores.get("security", 0)
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì „ì²´ ì ìˆ˜ ê³„ì‚°
        weights = {
            "functionality": 0.3,
            "quality": 0.25,
            "architecture": 0.2,
            "testability": 0.15,
            "security": 0.1
        }
        
        self.metrics["overall_score"] = sum(
            scores.get(dim, 0) * weight 
            for dim, weight in weights.items()
        )
        
        return self.metrics
```markdown

#### 2. ê°œì„  ì¶”ì  ì§€í‘œ
```python
class ImprovementTracker:
    def __init__(self):
        self.improvement_history = []
        self.quality_trends = []
    
    def track_improvement(self, before_review, after_review):
        improvement_data = {
            "before_score": before_review.get("overall_score", 0),
            "after_score": after_review.get("overall_score", 0),
            "improvement": after_review.get("overall_score", 0) - before_review.get("overall_score", 0),
            "timestamp": datetime.now()
        }
        
        self.improvement_history.append(improvement_data)
        self.update_quality_trends()
        
        return improvement_data
    
    def get_improvement_statistics(self):
        if not self.improvement_history:
            return {"average_improvement": 0, "improvement_rate": 0}
        
        improvements = [data["improvement"] for data in self.improvement_history]
        
        return {
            "average_improvement": sum(improvements) / len(improvements),
            "improvement_rate": len([i for i in improvements if i > 0]) / len(improvements),
            "max_improvement": max(improvements),
            "min_improvement": min(improvements)
        }
```markdown

### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

```python
class QualityDashboard:
    def __init__(self, reviewer, tracker):
        self.reviewer = reviewer
        self.tracker = tracker
        self.dashboard_data = {}
    
    def generate_dashboard(self):
        dashboard_data = {
            "current_quality": self.get_current_quality_metrics(),
            "quality_trends": self.get_quality_trends(),
            "improvement_statistics": self.tracker.get_improvement_statistics(),
            "top_issues": self.get_top_issues(),
            "recommendations": self.get_recommendations()
        }
        
        return dashboard_data
    
    def get_current_quality_metrics(self):
        return {
            "overall_score": self.reviewer.get_latest_score(),
            "dimension_scores": self.reviewer.get_dimension_scores(),
            "pass_rate": self.reviewer.get_pass_rate(),
            "review_count": self.reviewer.get_review_count()
        }
    
    def get_quality_trends(self):
        return {
            "score_trend": self.tracker.get_score_trend(),
            "improvement_trend": self.tracker.get_improvement_trend(),
            "issue_frequency": self.tracker.get_issue_frequency()
        }
```markdown

## ğŸ› ï¸ ì‹¤ìŠµ: ê²€ì¦ ë£¨í”„ ì‹œìŠ¤í…œ êµ¬ì¶•

### í”„ë¡œì íŠ¸ ì„¤ì •

```bash
# í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
mkdir verification-loop-system
cd verification-loop-system

# ê°€ìƒí™˜ê²½ ì„¤ì •
python -m venv venv
source venv/bin/activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install openai python-dotenv matplotlib seaborn
```markdown

### ë©”ì¸ ì‹œìŠ¤í…œ êµ¬í˜„

```python
# main.py
import os
from dotenv import load_dotenv
from verification_system import VerificationSystem

def main():
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    
    # ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    verification_system = VerificationSystem(
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )
    
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    test_code = """
    function calculateTotal(items) {
        let total = 0;
        for (let item of items) {
            total += item.price * item.quantity;
        }
        return total;
    }
    """
    
    requirements = "ì¥ë°”êµ¬ë‹ˆ ì´ ê¸ˆì•¡ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜"
    
    # ê²€ì¦ ìˆ˜í–‰
    result = verification_system.review_and_improve(test_code, requirements)
    
    print("ê²€ì¦ ê²°ê³¼:")
    print(f"í†µê³¼: {result['passed']}")
    print(f"ì „ì²´ ì ìˆ˜: {result['overall_score']}")
    print(f"í”¼ë“œë°±: {result['feedback']}")
    
    if not result['passed']:
        print("\nê°œì„ ëœ ì½”ë“œ:")
        print(result['improved_code'])

if __name__ == "__main__":
    main()
```

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ì´ ê°€ì´ë“œë¥¼ ì™„ë£Œí•œ í›„ì—ëŠ” ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì„¸ìš”:

1. **[1-8: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ í”„ë ˆì„ì›Œí¬ ì„ íƒ](1-8-orchestration-framework.md)**
2. **[1-9: CrewAIë¡œ ì²« ë²ˆì§¸ íŒ€ ë¹Œë”©](1-9-crewai-team-building.md)**

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [Code Review Best Practices](https://code-review.dev/)
- [Quality Assurance Guidelines](https://qa-guidelines.dev/)
- [Automated Testing Strategies](https://automated-testing.dev/)

---

**"í’ˆì§ˆì€ ê²€ì¦ì—ì„œ ë‚˜ì˜¨ë‹¤"** - ê²€ì¦ ë£¨í”„ì˜ í•µì‹¬ ì² í•™
